{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a688528",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "from tqdm.notebook import tqdm\n",
    "from knockknock import slack_sender\n",
    "import os\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "webhook_url = os.environ['KNOCKKNOCK_WEBHOOK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395ae1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_twitter_single_search(bank, search_term, start_date, end_date):\n",
    "    # pytz to localize the date\n",
    "    utc=pytz.UTC\n",
    "    \n",
    "    # Converting start_date and end_date to datetime objects\n",
    "    start_date = utc.localize(datetime.strptime(start_date, \"%Y-%m-%d\"))\n",
    "    end_date = utc.localize(datetime.strptime(end_date, \"%Y-%m-%d\"))\n",
    "\n",
    "    # Creating list to append tweet data to\n",
    "    tweets_list = []\n",
    "\n",
    "    # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'{search_term} since:{start_date:%Y-%m-%d} until:{end_date:%Y-%m-%d} lang:en').get_items()):\n",
    "        \n",
    "        # Checking if tweet date is before start_date \n",
    "        if tweet.date < start_date:\n",
    "            break\n",
    "\n",
    "        tweets_list.append([tweet.date, tweet.id, tweet.rawContent, tweet.user.username, tweet.replyCount, tweet.retweetCount, tweet.likeCount])\n",
    "\n",
    "    # Creating a dataframe from the tweets list above\n",
    "    tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet_Id', 'Tweet', 'Username', 'Reply_Count', 'Retweet_Count', 'Like_Count'])\n",
    "    \n",
    "    # Adding Bank column to the dataframe\n",
    "    tweets_df[\"Bank\"] = bank\n",
    "    \n",
    "    return tweets_df\n",
    "\n",
    "@slack_sender(webhook_url=webhook_url, channel=\"#general\")\n",
    "def scrap_twitter_multiple_search(bank_dict, start_date, end_date):\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    # Loop through banks\n",
    "    for bank in tqdm(bank_dict.keys(), total=len(bank_dict)):\n",
    " \n",
    "        # Loop through each search term per bank\n",
    "        for search_term in tqdm(bank_dict[bank], total=len(bank_dict[bank])):\n",
    "            dfs.append(scrap_twitter_single_search(bank, search_term, start_date, end_date))\n",
    "\n",
    "    # Concatenate multiple dataframes and drop duplicate tweets\n",
    "    result = pd.concat(dfs).drop_duplicates()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b915322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = '2019'\n",
    "end_year = '2020'\n",
    "\n",
    "# bank_dict = {'fnb': ['fnb', 'FNBSA', 'fnbSouthAfrica'],\n",
    "#              'absa': ['absa', 'absaSA', 'ABSASouthAfrica'],\n",
    "#              'nedbank': ['nedbank', 'NEDBANKSA', 'nedbankSouthAfrica'],\n",
    "#              'capitec': ['capitec', 'CapitecBank', 'capitecSA'],\n",
    "#              'standard_bank': ['standard bank','standardbank', 'StandardbankSA']}\n",
    "\n",
    "\n",
    "bank_dict = {'capitec': ['capitecbankSA'],\n",
    "             'standard_bank': ['StandardbankZA', 'standardbankSouthAfrica']}\n",
    "\n",
    "\n",
    "# pytz to localize the date\n",
    "utc=pytz.UTC\n",
    "\n",
    "start_date = f'{start_year}-01-01'\n",
    "end_date = f'{end_year}-12-31'\n",
    "\n",
    "\n",
    "# Converting start_date and end_date to datetime objects\n",
    "start_date = utc.localize(datetime.strptime(start_date, \"%Y-%m-%d\"))\n",
    "end_date = utc.localize(datetime.strptime(end_date, \"%Y-%m-%d\"))\n",
    "current_date = start_date\n",
    "\n",
    "while current_date < end_date:\n",
    "    next_date = current_date + relativedelta(years=1)\n",
    "    print(f'Scrapping data from {current_date.strftime(\"%Y-%m-%d\")} to {next_date.strftime(\"%Y-%m-%d\")}')\n",
    "    df = scrap_twitter_multiple_search(bank_dict, current_date.strftime(\"%Y-%m-%d\"), next_date.strftime(\"%Y-%m-%d\"))\n",
    "    df = df.astype({'Tweet_Id': int, 'Tweet':'string', 'Username': 'string', 'Reply_Count':int, 'Retweet_Count':int, 'Like_Count':int, 'Bank':'string'})\n",
    "    df.to_parquet(f\"updated_search_terms_tweets_from_{current_date.strftime('%Y-%m-%d')}_to_{next_date.strftime('%Y-%m-%d')}.parquet\")\n",
    "    current_date = next_date"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "3783e187a3956093a2d91726783b18164bbef240ac0a90eac75ec59d8f0bbb5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
